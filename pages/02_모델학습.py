# pages/02_ëª¨ë¸_í•™ìŠµ.py
# í•™ìŠµ íŒŒë¼ë¯¸í„° ì €ìž¥/ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥ì´ ì¶”ê°€ëœ YOLO ëª¨ë¸ í•™ìŠµ íŽ˜ì´ì§€

import streamlit as st
import zipfile
import os
from datetime import datetime
from utils.training import run_training
from utils.logger import log_training
from utils.config import load_config, save_config
import matplotlib.pyplot as plt
import pandas as pd
import glob
from PIL import Image
import yaml

# í•™ìŠµ ê²°ê³¼ ì €ìž¥ ê²½ë¡œ ì„¤ì •
RESULTS_DIR = "runs/detect"  # ê¸°ë³¸ ê²°ê³¼ ì €ìž¥ ë””ë ‰í† ë¦¬
os.makedirs(RESULTS_DIR, exist_ok=True)

# íŽ˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="YOLO ëª¨ë¸ í•™ìŠµ", page_icon="ðŸ§ ", layout="wide")

# ì‚¬ì´ë“œë°”ì— ì„¤ì • ì €ìž¥ ì„¹ì…˜ ì¶”ê°€
with st.sidebar:
    st.subheader("âš™ï¸ í•™ìŠµ ì„¤ì • ì €ìž¥")
    save_as_preset = st.text_input(
        "ì„¤ì • ì´ë¦„",
        placeholder="ì €ìž¥í•  ì„¤ì • ì´ë¦„ ìž…ë ¥...",
        help="í˜„ìž¬ ì„¤ì •ì„ ì €ìž¥í•  ì´ë¦„ì„ ìž…ë ¥í•˜ì„¸ìš”",
    )

    if st.button("ðŸ’¾ í˜„ìž¬ ì„¤ì • ì €ìž¥", use_container_width=True):
        if save_as_preset.strip():  # ì„¤ì • ì´ë¦„ì´ ë¹„ì–´ìžˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì €ìž¥
            current_training = {
                "model_arch": model_arch,
                "epochs": epochs,
                "batch": batch,
                "imgsz": imgsz,
                "optimizer": optimizer,
                "learning_rate": learning_rate,
                "project_name": project_name,
            }
            save_config("training", current_training)
            st.success("âœ… í•™ìŠµ ì„¤ì •ì´ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("âš ï¸ ì„¤ì • ì´ë¦„ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”.")

    st.divider()

# íŽ˜ì´ì§€ í—¤ë” ì„¤ì •
st.title("ðŸ§  YOLO ëª¨ë¸ í•™ìŠµ")

# í˜„ìž¬ ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
training_config = load_config("training")  # í•™ìŠµ ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°

# í•™ìŠµ ì„¤ì •ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
if not training_config:
    training_config = {
        "model_arch": "yolov8n.pt",
        "epochs": 100,
        "batch": 16,
        "imgsz": 640,
        "optimizer": "Adam",
        "learning_rate": 0.001,
        "project_name": "ocr_digit",
    }

# í˜„ìž¬ ì„¤ì •ëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
with st.expander("í˜„ìž¬ ëª¨ë¸ ì„¤ì • ì •ë³´", expanded=False):
    st.info(
        f"""
        **í˜„ìž¬ ëª¨ë¸ ì„¤ì •:**
        - ëª¨ë¸ ê²½ë¡œ: {training_config.get('model_path', 'yolov8n.pt')}
        """
    )

# ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì„¹ì…˜
st.subheader("ðŸ“¦ í•™ìŠµìš© ë°ì´í„°ì…‹")


# data.yamlì˜ ê²½ë¡œ êµ¬ì¡° ê²€ì¦ ë° ìˆ˜ì •
def validate_dataset_structure(dataset_dir):
    """ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ê²€ì¦í•˜ê³  í•„ìš”í•œ ê²½ìš° ìˆ˜ì •í•©ë‹ˆë‹¤."""
    yaml_path = os.path.join(dataset_dir, "data.yaml")
    if not os.path.exists(yaml_path):
        st.error("âŒ data.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    with open(yaml_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    # í•„ìˆ˜ í‚¤ í™•ì¸
    required_keys = ["train", "val", "nc", "names"]
    missing_keys = [key for key in required_keys if key not in data]
    if missing_keys:
        st.error(f"âŒ data.yamlì— í•„ìˆ˜ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_keys)}")
        return False

    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸
    images_dir = os.path.join(dataset_dir, "images")
    labels_dir = os.path.join(dataset_dir, "labels")

    for dir_type in ["train", "val"]:
        img_dir = os.path.join(images_dir, dir_type)
        label_dir = os.path.join(labels_dir, dir_type)

        if not os.path.exists(img_dir):
            os.makedirs(img_dir, exist_ok=True)
        if not os.path.exists(label_dir):
            os.makedirs(label_dir, exist_ok=True)

        # data.yamlì˜ ê²½ë¡œ ì—…ë°ì´íŠ¸
        data[dir_type] = os.path.join(images_dir, dir_type)

    # data.yaml ì—…ë°ì´íŠ¸
    with open(yaml_path, "w", encoding="utf-8") as f:
        yaml.dump(data, f, allow_unicode=True)

    return True


def validate_dataset_for_training(dataset_dir):
    """
    í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ì…‹ êµ¬ì¡°ì™€ ë°ì´í„°ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    validation_results = {
        "is_valid": True,
        "messages": [],
        "stats": {
            "train": {"images": 0, "labels": 0},
            "val": {"images": 0, "labels": 0},
        },
    }

    # 1. data.yaml íŒŒì¼ ê²€ì¦
    yaml_path = os.path.join(dataset_dir, "data.yaml")
    if not os.path.exists(yaml_path):
        validation_results["is_valid"] = False
        validation_results["messages"].append("âŒ data.yaml íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return validation_results

    # 2. yaml íŒŒì¼ ë‚´ìš© ê²€ì¦
    try:
        with open(yaml_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
            required_keys = ["train", "val", "nc", "names"]
            missing_keys = [key for key in required_keys if key not in data]
            if missing_keys:
                validation_results["is_valid"] = False
                validation_results["messages"].append(
                    f"âŒ data.yamlì— í•„ìˆ˜ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_keys)}"
                )
    except Exception as e:
        validation_results["is_valid"] = False
        validation_results["messages"].append(f"âŒ data.yaml íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        return validation_results

    # 3. ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸
    for split in ["train", "val"]:
        img_dir = os.path.join(dataset_dir, "images", split)
        label_dir = os.path.join(dataset_dir, "labels", split)

        if not os.path.exists(img_dir) or not os.path.exists(label_dir):
            validation_results["is_valid"] = False
            validation_results["messages"].append(
                f"âŒ {split} í´ë” êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
            )
            continue

        # 4. ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ìˆ˜ í™•ì¸
        images = (
            glob.glob(os.path.join(img_dir, "*.jpg"))
            + glob.glob(os.path.join(img_dir, "*.jpeg"))
            + glob.glob(os.path.join(img_dir, "*.png"))
        )
        labels = glob.glob(os.path.join(label_dir, "*.txt"))

        validation_results["stats"][split]["images"] = len(images)
        validation_results["stats"][split]["labels"] = len(labels)

        if len(images) == 0:
            validation_results["is_valid"] = False
            validation_results["messages"].append(f"âŒ {split} ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if len(labels) == 0:
            validation_results["is_valid"] = False
            validation_results["messages"].append(f"âŒ {split} ë¼ë²¨ì´ ì—†ìŠµë‹ˆë‹¤.")

        if len(images) != len(labels):
            validation_results["messages"].append(
                f"âš ï¸ {split} ì´ë¯¸ì§€({len(images)})ì™€ ë¼ë²¨({len(labels)}) ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤!"
            )

    # 5. ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ í™•ì¸
    total_images = (
        validation_results["stats"]["train"]["images"]
        + validation_results["stats"]["val"]["images"]
    )

    if total_images > 0:
        val_ratio = validation_results["stats"]["val"]["images"] / total_images
        if val_ratio < 0.1:  # ê²€ì¦ ë°ì´í„°ê°€ 10% ë¯¸ë§Œì¸ ê²½ìš°
            validation_results["messages"].append(
                f"âš ï¸ ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤ ({val_ratio:.1%}). 10% ì´ìƒì„ ê¶Œìž¥í•©ë‹ˆë‹¤."
            )

    return validation_results


uploaded_file = st.file_uploader("ðŸ”¼ ZIP í˜•ì‹ì˜ í•™ìŠµ ë°ì´í„°ì…‹ ì—…ë¡œë“œ", type=["zip"])
if uploaded_file:
    dataset_dir = "dataset"
    # ê¸°ì¡´ ë°ì´í„°ì…‹ ì‚­ì œ
    if os.path.exists(dataset_dir):
        import shutil

        shutil.rmtree(dataset_dir)
    os.makedirs(dataset_dir, exist_ok=True)

    # ì••ì¶• í•´ì œ
    with zipfile.ZipFile(uploaded_file, "r") as zip_ref:
        zip_ref.extractall(dataset_dir)

    # ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦
    if not validate_dataset_structure(dataset_dir):
        st.error("âŒ ë°ì´í„°ì…‹ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        st.info(
            """
        í•„ìš”í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡°:
        dataset/
        â”œâ”€â”€ data.yaml
        â”œâ”€â”€ images/
        â”‚   â”œâ”€â”€ train/
        â”‚   â””â”€â”€ val/
        â””â”€â”€ labels/
            â”œâ”€â”€ train/
            â””â”€â”€ val/
        """
        )
        st.stop()

    st.session_state.uploaded_filename = uploaded_file.name
    st.success("âœ… ì••ì¶• í•´ì œ ì™„ë£Œ")

    # ðŸ–¼ï¸ ìƒ˜í”Œ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°
    st.subheader("ðŸ–¼ï¸ ìƒ˜í”Œ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°")
    image_files = glob.glob(os.path.join(dataset_dir, "images", "*.jpg")) + glob.glob(
        os.path.join(dataset_dir, "images", "*.png")
    )

    if image_files:
        cols = st.columns(3)
        for i, img_path in enumerate(image_files[:3]):
            img = Image.open(img_path)
            cols[i].image(img, caption=os.path.basename(img_path), width=300)
    else:
        st.info("ë¯¸ë¦¬ë³¼ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. (images/ í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”)")

    # ðŸ“‚ ë””ë ‰í† ë¦¬ ë¯¸ë¦¬ë³´ê¸°
    st.subheader("ðŸ“‚ ë°ì´í„°ì…‹ êµ¬ì¡°")
    for root, dirs, files in os.walk(dataset_dir):
        indent = "&nbsp;&nbsp;&nbsp;&nbsp;" * (
            root.count(os.sep) - dataset_dir.count(os.sep)
        )
        st.markdown(
            f"{indent}ðŸ“‚ `{os.path.relpath(root, dataset_dir)}`", unsafe_allow_html=True
        )
        for f in files:
            st.markdown(
                f"{indent}&nbsp;&nbsp;&nbsp;&nbsp;ðŸ“„ `{f}`", unsafe_allow_html=True
            )

# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì • í¼
st.subheader("ðŸ’¾ í•™ìŠµ ì„¤ì •")

# ëª¨ë¸ ì„ íƒ
model_arch = st.selectbox(
    "YOLO ëª¨ë¸",
    ["yolov8n.pt", "yolov8s.pt", "yolov8m.pt", "yolov8l.pt", "yolov8x.pt"],
    index=(
        ["yolov8n.pt", "yolov8s.pt", "yolov8m.pt", "yolov8l.pt", "yolov8x.pt"].index(
            training_config.get("model_arch", "yolov8n.pt")
        )
        if training_config.get("model_arch")
        in ["yolov8n.pt", "yolov8s.pt", "yolov8m.pt", "yolov8l.pt", "yolov8x.pt"]
        else 0
    ),
    help="YOLO ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤...",  # ê¸°ì¡´ help í…ìŠ¤íŠ¸ ìœ ì§€
)

# Epochsì™€ Batch Sizeë¥¼ í•œ ì¤„ì—
col1, col2 = st.columns(2)
with col1:
    epochs = st.slider(
        "Epoch ìˆ˜",
        1,
        300,
        value=int(training_config.get("epochs", 100)),
        help="ì „ì²´ ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí• ì§€ ì„¤ì •í•©ë‹ˆë‹¤...",  # ê¸°ì¡´ help í…ìŠ¤íŠ¸ ìœ ì§€
    )
with col2:
    batch = st.slider(
        "Batch Size",
        1,
        64,
        value=int(training_config.get("batch", 16)),
        help="í•œ ë²ˆì— ì²˜ë¦¬í•  ì´ë¯¸ì§€ ê°œìˆ˜ìž…ë‹ˆë‹¤...",  # ê¸°ì¡´ help í…ìŠ¤íŠ¸ ìœ ì§€
    )

# ì´ë¯¸ì§€ í¬ê¸°, Optimizer, Learning Rateë¥¼ í•œ ì¤„ì—
col3, col4, col5 = st.columns(3)
with col3:
    imgsz = st.selectbox(
        "ì´ë¯¸ì§€ í¬ê¸°",
        [416, 512, 640],
        index=(
            [416, 512, 640].index(training_config.get("imgsz", 640))
            if training_config.get("imgsz") in [416, 512, 640]
            else 2
        ),
        help="í•™ìŠµì— ì‚¬ìš©í•  ì´ë¯¸ì§€ í¬ê¸°(í”½ì…€)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤...",  # ê¸°ì¡´ help í…ìŠ¤íŠ¸ ìœ ì§€
    )
with col4:
    optimizer = st.selectbox(
        "Optimizer",
        ["SGD", "Adam"],
        index=(
            ["SGD", "Adam"].index(training_config.get("optimizer", "Adam"))
            if training_config.get("optimizer") in ["SGD", "Adam"]
            else 1
        ),
        help="í•™ìŠµì— ì‚¬ìš©í•  ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•©ë‹ˆë‹¤...",  # ê¸°ì¡´ help í…ìŠ¤íŠ¸ ìœ ì§€
    )
with col5:
    learning_rate = st.number_input(
        "Learning Rate",
        value=float(training_config.get("learning_rate", 0.001)),
        format="%f",
        min_value=0.0001,
        max_value=0.1,
        step=0.0001,
        help="ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” í•™ìŠµë¥ ìž…ë‹ˆë‹¤...",  # ê¸°ì¡´ help í…ìŠ¤íŠ¸ ìœ ì§€
    )

# ë””ë°”ì´ìŠ¤ ì„ íƒ ì˜µì…˜ ì¶”ê°€
device = st.selectbox(
    "í•™ìŠµ ë””ë°”ì´ìŠ¤",
    options=["cpu", "cuda"],
    index=0,
    help="í•™ìŠµì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n\n"
    "- CPU: ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜ í•™ìŠµ ì†ë„ê°€ ëŠë¦¼\n"
    "- CUDA: NVIDIA GPUê°€ ìžˆëŠ” ê²½ìš° ì„ íƒ. ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥",
)

# í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •
project_name = st.text_input(
    "í”„ë¡œì íŠ¸ ì´ë¦„",
    value=training_config.get("project_name", "ocr_digit"),
    help="í•™ìŠµ ê²°ê³¼ê°€ ì €ìž¥ë  í”„ë¡œì íŠ¸ í´ë”ì˜ ì´ë¦„ìž…ë‹ˆë‹¤...",  # ê¸°ì¡´ help í…ìŠ¤íŠ¸ ìœ ì§€
)

# í•™ìŠµ ì‹œìž‘ ë²„íŠ¼
if st.button("ðŸš€ í•™ìŠµ ì‹œìž‘", use_container_width=True):
    try:
        # ë°ì´í„°ì…‹ ê²€ì¦
        validation_results = validate_dataset_for_training("dataset")

        if not validation_results["is_valid"]:
            st.error("âŒ ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨")
            for msg in validation_results["messages"]:
                st.error(msg)
            st.stop()

        # YOLO ëª…ë ¹ì–´ ìƒì„±
        data_yaml = os.path.abspath("dataset/data.yaml")
        yolo_cmd = (
            f"yolo task=detect mode=train model={model_arch} data={data_yaml} "
            f"epochs={epochs} batch={batch} imgsz={imgsz} lr0={learning_rate} "
            f"optimizer={optimizer.lower()} project={RESULTS_DIR} name={project_name} "
            f"device={device} verbose=True"  # device ì˜µì…˜ ìˆ˜ì •
        )

        # í•™ìŠµ ì‹¤í–‰
        with st.spinner("ðŸš€ í•™ìŠµ ì§„í–‰ ì¤‘..."):
            start_time = datetime.now()
            st.info(
                "í•™ìŠµì´ ì‹œìž‘ë˜ì—ˆìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”."
            )

            # YOLO í•™ìŠµ ì‹¤í–‰
            result = run_training(yolo_cmd)

            # í‘œì¤€ ì¶œë ¥ ì²˜ë¦¬ ê°œì„ 
            if result.stdout:
                if hasattr(result.stdout, "decode"):
                    print(result.stdout.decode("utf-8", errors="ignore"))
                else:
                    print(result.stdout)

            # ì˜¤ë¥˜ ì¶œë ¥ í™•ì¸
            if result.stderr:
                if hasattr(result.stderr, "decode"):
                    error_msg = result.stderr.decode("utf-8", errors="ignore")
                    print("Error output:", error_msg)
                else:
                    print("Error output:", result.stderr)

            end_time = datetime.now()
            duration = end_time - start_time

            if result.returncode == 0:
                # í•™ìŠµ ê²°ê³¼ ì €ìž¥
                save_path = os.path.join(
                    RESULTS_DIR, project_name, "weights", "best.pt"
                )
                if os.path.exists(save_path):
                    # detection ì„¤ì • ì—…ë°ì´íŠ¸
                    detection_config = load_config("detection") or {}
                    detection_config["model_path"] = save_path
                    save_config("detection", detection_config)

                    st.success(
                        f"""âœ… í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
                    - ì†Œìš” ì‹œê°„: {duration}
                    - ëª¨ë¸ ì €ìž¥ ìœ„ì¹˜: {save_path}"""
                    )
                else:
                    st.warning("âš ï¸ í•™ìŠµì€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.error("âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                if result.stderr:
                    st.code(result.stderr.decode(), language="bash")

    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
